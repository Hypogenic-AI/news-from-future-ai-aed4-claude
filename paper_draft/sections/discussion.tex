\section{Discussion}
\label{sec:discussion}

\para{Market anchoring is essential, not optional.} Our results paint a stark picture of LLM forecasting capabilities. On \halawi, where community predictions are available as anchors, \gptfour achieves near-human accuracy (Brier 0.060 vs.\ 0.058). On \kalshibench, where no anchors are available, the same model performs at chance level (0.254 vs.\ \naive 0.250). This gap---far larger than the 17--28\% improvement reported by \citet{schoenegger2024wisdom} for GPT-4 and Claude~2---suggests that newer, more capable models may actually benefit \emph{more} from market anchoring, possibly because they are better at integrating external evidence into their reasoning. Our 29.2\% improvement is consistent with this interpretation.

\para{The forecasting--generation synergy.} The combination of prediction markets and LLMs is genuinely synergistic. Prediction markets excel at producing calibrated probabilities but communicate them only as numbers. LLMs excel at producing fluent narratives but lack calibrated uncertainty awareness~\citep{smith2025kalshibench}. By feeding market-calibrated probabilities into the generation process, \futurenews produces articles that are both linguistically compelling and epistemically appropriate. The significant improvement in uncertainty handling (Cohen's $d{=}0.77$) when probability information is provided confirms that LLMs can modulate their narrative tone---but only when given explicit guidance about how confident to be.

\para{Limitations.}
We identify six limitations of the current study.

\emph{LLM-as-judge bias.} Using \gptfour to evaluate articles generated by \gptfour introduces self-evaluation bias~\citep{zheng2023judging}. The near-ceiling scores (4.28--5.00 across all conditions) likely reflect this. A human evaluation study would provide more discriminating assessments and is an important direction for future work.

\emph{Ceiling effects.} Most quality scores cluster at 4.5--5.0, making it difficult to distinguish quality differences between conditions except on uncertainty handling. A more granular rubric or adversarial evaluation would be more informative.

\emph{Single model.} We evaluate only \gptfour for both forecasting and generation. Testing additional models (e.g., Claude~4.5, Gemini~2.5~Pro) would strengthen generalizability claims.

\emph{Retrospective evaluation.} We evaluate on historical questions with known resolutions. True validation of future news requires prospective evaluation where articles are generated before event resolution and assessed afterward. Our live pipeline (\secref{sec:exp3}) is a step in this direction but does not yet include temporal validation.

\emph{Sample size.} With 200 questions per forecasting dataset and 50 article pairs, our study has moderate statistical power. The significant results (all $p < 0.05$) are encouraging, but larger samples would narrow confidence intervals and permit finer-grained analyses (\eg by topic category).

\emph{Ethical risks.} Plausible future news articles could be misused for market manipulation, political influence, or the generation of misinformation that is difficult to distinguish from real reporting. Any deployed system must include prominent disclaimers, watermarking, and content provenance metadata. We discuss these considerations further in the broader impact section of the appendix.

\para{Practical feasibility.}
Despite these limitations, the results suggest that a production ``News from the Future'' service is technically feasible with current technology. The required components---prediction market APIs, LLM APIs, and HTML rendering---are all readily available. The critical design decision is the anchoring mechanism: our results indicate that LLMs should not forecast independently but should treat market prices as the primary input, contributing only marginal adjustments based on their own reasoning.
